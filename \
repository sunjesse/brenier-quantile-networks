import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import torch.optim as optim
import argparse
import numpy as np
import scipy
from scipy.stats import norm
#our libs
from lib import radam

class QNN(nn.Module):
    def __init__(self):
        super(QNN, self).__init__()
        self.w1 = nn.Linear(1, 5)
        self.bn1 = nn.BatchNorm1d(5)
        self.w2 = nn.Linear(5, 5)
        self.bn2 = nn.BatchNorm2d(5)
        self.w3 = nn.Linear(5, 1)

    def forward(self, x):
        x = F.relu(self.bn1(self.w1(x)))
        x = F.relu(self.bn2(self.w2(x)))
        x = self.w3(x)
        return x

def dataloader(args):
    transform = transforms.Compose(
	[transforms.ToTensor(),
	 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
					    download=True, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,
					      shuffle=True, num_workers=2)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False,
					   download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size,
					     shuffle=False, num_workers=2)

    return trainloader, testloader

def optimizer(net, args):
    assert args.optimizer.lower() in ["sgd", "adam", "radam"], "Invalid Optimizer"

    if args.optimizer.lower() == "sgd":
	       return optim.SGD(net.parameters(), lr=args.lr, momentum=args.beta1, nesterov=args.nesterov)
    elif args.optimizer.lower() == "adam":
	       return optim.Adam(net.parameters(), lr=args.lr, betas=(args.beta1, args.beta2))
    elif args.optimizer.lower() == "radam":
           return radam.RAdam(net.parameters(), lr=args.lr, betas=(args.beta1, args.beta2))
 
def criterion(pred, label, quantile):
    loss = (label-pred)*(quantile - (label-pred < 0).float32())
    return torch.mean(loss)

def test(net, criterion, args):
    net.eval()
    with torch.no_grad():
        correct = 0
        for i in range(100):
            inputs = np.random.uniform(0, 1, size=(args.batch_size, 1))
            labels = scipy.stats.norm.ppf(inputs)
            inputs, labels = torch.from_numpy(inputs), torch.from_numpy(labels)
            outputs = net(inputs)
            pred = F.softmax(outputs, 1)
            _, pred = torch.max(pred, 1)
        print("Test set accuracy: " + str(float(correct)/ float(testloader.__len__() * args.batch_size)))

def train(net, optimizer, args):
    for epoch in range(1, args.epoch+1):
        running_loss = 0.0
        for i in range(args.iters):
            inputs = np.random.uniform(0, 1, size=(args.batch_size, 1))
            labels = norm.ppf(inputs)
            inputs, labels = torch.from_numpy(inputs), torch.from_numpy(labels)
            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels, inputs)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
        print('[Epoch %02d, Loss: %.5f' %
			(epoch, running_loss))
        running_loss = 0.0
        #test(nest, criterion, args)

if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    # optimization related arguments
    parser.add_argument('--batch_size', default=64, type=int,
                        help='input batch size')
    parser.add_argument('--epoch', default=10, type=int,
                        help='epochs to train for')
    parser.add_argument('--optimizer', default='sgd', help='optimizer')
    parser.add_argument('--lr', default=0.001, type=float, help='LR')
    parser.add_argument('--beta1', default=0.9, type=float,
                        help='momentum for sgd, beta1 for adam')
    parser.add_argument('--beta2', default=0.999, type=float)
    parser.add_argument('--nesterov', default=False)
    parser.add_argument('--iters', default=25)

    args = parser.parse_args()

    print("Input arguments:")
    for key, val in vars(args).items():
        print("{:16} {}".format(key, val))

    #trainloader, testloader = dataloader(args)
    net = QNN()
    #criterion = nn.CrossEntropyLoss()
    optimizer = optimizer(net, args)
    train(net, optimizer, args)
    #test(net, criterion, testloader, args)
    print("Training completed!")
